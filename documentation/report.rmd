# Distribution map using survey data
## An example of Starry ray, using INLA

Most of the data we collect at sea is spatially and temporally correlated, such as IBTS or BTS survey data.

The statistical package INLA has the advantage over other software in R that it can combine spatial, temporal, zero-inflated, random effects etc models all in one. It therefore is very powerful and may become your one-stop-to-go package. 

##Preparing the R environment

First, we need to set the path where the data is located and to load the relevant libraries. We need the inla package, but also a number of packages for spatial analyses and for plotting the data and the results.

```{r, eval=T, echo=FALSE}
path <- "d://WMR-INLA/data"

library(INLA); library(fields); library(lattice); library(latticeExtra); library(grid); library(gridExtra);library(rgdal); library(mapdata) 

```


##The IBTS and BTS data





First we read in the IBTS dataset. Thas dataset contains the CPUE per length per haul can be dowloaded from datras.ices.dk. In this case we downloaded the data for starry ray. This data set has zero hauls (hauls where no individuals are caught) are coded so that Lengthclass is equal to zero and CPUE number per hour is zero.   

Note that we should consider downloading the data straight from GITHUB


```{r, eval=T, echo=TRUE}
```

```{r, eval=T, echo=TRUE}
cpue_IBTS <- read.csv(file.path(path,"CPUE per length per haul_2017-07-12 11_18_36.csv"), stringsAsFactors = F)
cpue_IBTS$NoPerHaul <- round(cpue_IBTS$CPUE_number_per_hour*(cpue_IBTS$HaulDur/60))
cpue_IBTS <- cpue_IBTS[!(names(cpue_IBTS) %in% c("DateofCalculation", "Area", "SubArea", "DayNight","Species","Sex","DateTime","CPUE_number_per_hour"))]
```

We have a similar but slightly different data set for BTS

```{r, eval=T, echo=TRUE}
cpue_BTS  <- read.csv(file.path(path,"CPUE per length per Hour and Swept Area_2017-07-12 11_29_36.csv"), stringsAsFactors = F)
cpue_BTS <- cpue_BTS[!(names(cpue_BTS) %in% c("Country","DoorType","HaulLat","HaulLong","StatRec","DataType","Rigging","Tickler","Distance","Warplngt","TowDir","WindDir","WindSpeed","SwellDir","SwellHeight","StNo","Month","Day","TimeShot","Stratum","ICESArea","DateofCalculation", "Area", "SubArea", "DayNight","Species", "Sex","SpecVal","SubFactor","GearExp","SweepLngt","Netopening","BycSpecRecCode","StdSpecRecCode","HaulVal","CPUE_number_per_hour","CPUE_number_per_km2","SweptArea_km2","DistanceDerived","BeamWidth", "HLNoAtLngt"))]
cpue_BTS[is.na(cpue_BTS$LngtClass),]$NoPerHaul <- 0
cpue_BTS[is.na(cpue_BTS$LngtClass),]$LngtClass <- 0
```


Combine the two sets into one
```{r, eval=T, echo=T}
cpue <- rbind(cpue_BTS,cpue_IBTS)
```
# Make unique haul for trawllist and so that we can do different merges needed later

```{r, eval=T, echo=T}
cpue$uniqueHaul <-   paste(cpue$Year, cpue$Survey, cpue$Quarter, cpue$Ship, cpue$HaulNo, cpue$ShootLat, cpue$ShootLon, sep="_")
trawllist <- cpue[!duplicated(cpue$uniqueHaul),!names( cpue) %in% c("Species","AphiaID","NoPerHaul","Sex", "LngtClass")]

#expand grid 
haulsLengths <- expand.grid(uniqueHaul=unique(cpue$uniqueHaul),LngtClass=unique(cpue$LngtClass), stringsAsFactors = F)
nrow(haulsLengths)
full_cpue <- merge(haulsLengths,cpue[names(cpue) %in% c("uniqueHaul", "LngtClass","NoPerHaul")], all.x=T)
rm(haulsLengths)
head(full_cpue)
```

```{r, eval=T, echo=T}
full_cpue[is.na(full_cpue$NoPerHaul),]$NoPerHaul <- 0
nrow(full_cpue)
full_cpue  <- merge(full_cpue,trawllist, all.x=T)
nrow(full_cpue)
```

```{r, eval=T, echo=T}
#now remove zero lengths
full_cpue <- full_cpue[full_cpue$LngtClass> 0,]
```

```{r, eval=T, echo=T}
# make selection of  fullset.
cpue_subset <- full_cpue[full_cpue$Year >2014,]
nrow(cpue_subset)
```

```{r, eval=T, echo=T}
cpue_subset <- aggregate(NoPerHaul~uniqueHaul+Survey+  Quarter + Ship + Gear  + HaulNo + Year + HaulDur + ShootLat +  ShootLong+  Depth, data= cpue_subset,FUN="sum")
```


```{r, eval=T, echo=T}
plot(cpue_subset$ShootLong,cpue_subset$ShootLat, cex=(cpue_subset$NoPerHaul)/10+0.1, col=as.numeric(as.factor(cpue_subset$Survey)))
map("world",add=T)
```

```{r, eval=T, echo=T}
hist(cpue_subset$NoPerHaul, 200)
```



```{r, eval=T, echo=T}
#make year and survey as factor
cpue_subset[cpue_subset$Survey=="NS-IBTS",]$Survey <- "NSIBTS"


cpue_subset$fYear <- as.factor(cpue_subset$Year)
cpue_subset$fSurvey <- as.factor(cpue_subset$Survey)
```

```{r, eval=T, echo=T}

#en nu gaan we lekker verder typen

UTM <- project(cbind(cpue_subset$ShootLong, cpue_subset$ShootLat), "+proj=utm +zone=31U ellps=WGS84") 

cpue_subset$Xkm <- UTM[,1]/1000
cpue_subset$Ykm <- UTM[,2]/1000

Loc <- cbind(cpue_subset$Xkm , cpue_subset$Ykm )
Loc


ConvHull <- inla.nonconvex.hull(Loc, convex=-0.02, resolution=90)
mesh1    <- inla.mesh.2d(boundary = ConvHull,  max.edge=c(150))
plot(mesh1)
points(Loc, col = 1, pch = 16, cex = 0.5)

  # 2. Define the weighting factors a_ik (also called the projector matrix).
A1      <- inla.spde.make.A(mesh  = mesh1, loc   = Loc)
dim(A1)

  # 3. Define the spde
spde  <- inla.spde2.matern(mesh1)

w.st <- inla.spde.make.index('w', n.spde = spde$n.spde)
```

Note that in code below the names in the model matrix should not contain any special characters!

```{r, eval=T, echo=T}
  
# 5. Make a stack. In this process we tell INLA at which points on the mesh we sampled the response variable and the covariates. 

Xmatrix <- model.matrix(~  fYear + fSurvey +  HaulDur  ,  data = cpue_subset)

X <- as.data.frame(Xmatrix[,-1])

names(X) <- c(gsub("[:]",".",names(X)))

N <- nrow(cpue_subset)
Stack1 <- inla.stack(
    tag  = "Fit",
    data = list(y = cpue_subset$NoPerHaul),    
    A    = list(1,1, A1),                      
    effects = list( 
       Intercept=rep(1,N),
       X=X, #Covariates
       w=w.st))                  #Spatial field  



  fsp <- parse(text=c("y ~ -1 + Intercept + ",paste(c(names(X)," f(w, model = spde)"),collapse =" + ")))

  #INLA:::inla.dynload.workaround() 
  I1p <- inla(eval(fsp), family = "poisson", data=inla.stack.data(Stack1),
           control.compute = list(dic = TRUE, waic = TRUE),
           control.predictor = list(A = inla.stack.A(Stack1)))

  I1nb <- inla(eval(fsp), family = "nbinomial", data=inla.stack.data(Stack1),
           control.compute = list(dic = TRUE, waic = TRUE),
           control.predictor = list(A = inla.stack.A(Stack1)))

  
```

```{r, eval=T, echo=T}

summary(I1p)

summary(I1nb)

idx <- inla.stack.index(Stack1, tag= 'Fit')$data
quantile(I1nb$summary.fitted.values[idx,"mean"])
quantile(cpue_subset$NoPerHaul)

hist(I1nb$summary.fitted.values[idx,"mean"],200, xlim=c(0,50))
hist(cpue_subset$NoPerHaul,200, xlim=c(0,50))

```


```{r, eval=T, echo=T}

  wm <- I1nb$summary.random$w$mean
  wsd <- I1nb$summary.random$w$sd
  length(w)

  wproj <- inla.mesh.projector(mesh1,
                             xlim = range(Loc[,1]),
                             ylim = range(Loc[,2])) 

    wm.pm100100 <- inla.mesh.project(wproj,wm)
    wsd.pm100100 <- inla.mesh.project(wproj,wsd)
    
    grid <- expand.grid(x = wproj$x, y = wproj$y)
    grid$zm <- as.vector(wm.pm100100)   
    grid$zsd <- as.vector(wsd.pm100100)   


wld <- map('world', xlim=c(-5,15), ylim=c(47,62),plot=FALSE)
wld <- data.frame(lon=wld$x, lat=wld$y)

UTMmap <- project(cbind(wld$lon, wld$lat), "+proj=utm +zone=31U ellps=WGS84")
UTMmapFinal <- data.frame("xm"  =UTMmap[,1]/1e3, "ym" = UTMmap[,2]/1e3)

levelplot(zm ~ x * y  ,
          data = grid, 
          scales = list(draw = TRUE),
          xlab = list("Easting", cex = 1.5),
          ylab = list("Northing", cex = 1.5),
          main = list("Posterior mean spatial random fields", cex = 1.5),
          col.regions=tim.colors(25, alpha = 1),
          panel=function(x, y, z, subscripts,...){
            panel.levelplot(x, y, z, subscripts,...)
            grid.points(x = cpue_subset$Xkm,
                        y = cpue_subset$Ykm, 
                        pch = 1,
                        size = unit(cpue_subset$NoPerHaul/10, "char"))  
          }) +  xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black')

levelplot(zsd ~ x * y,
          data = grid, 
          scales = list(draw = TRUE),
          xlab = list("Easting", cex = 1.5),
          ylab = list("Northing", cex = 1.5),
          main = list("Posterior sd spatial random fields", cex = 1.5),
          col.regions=tim.colors(25, alpha = 1),
          panel=function(x, y, z, subscripts,...){
            panel.levelplot(x, y, z, subscripts,...)
            grid.points(x = cpue_subset$Xkm,
                        y = cpue_subset$Ykm, 
                        pch = 1,
                        size = unit(cpue_subset$NoPerHaul/10, "char"))  
          }) +  xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black')

  
```

```{r, eval=T, echo=T}

tcoo <- rbind(c(0.3, 0.3), c(0.5, 0.5), c(0.7, 0.7))
inla.nonconvex.hull
dim(Ap <- inla.spde.make.A(mesh = mesh1, loc = tcoo))
## [1] 3 505
x0 <- c(0.5, 0.5, 0.5)
#To do a fully Bayesian analysis, we include the target locations on the estimation process by
#assigning NA for the response at these locations. Defining the prediction stack
stk.pred <- inla.stack(tag='pred', A=list(Ap, 1), data=list(y=NA), ## response as NA
effects=list(s=1:spde$n.spde, data.frame(x=x0, b0=1)))
#Fit the model again with the full stack

stk.full <- inla.stack(stk.e, stk.pred)
p.res <- inla(formula, data=inla.stack.data(stk.full), ## full stack
control.predictor=list(compute=TRUE, ## compute the predictor
A=inla.stack.A(stk.full))) ## using full stack data
#Get the prediction data index and collect the PMD to work with
pred.ind <- inla.stack.index(stk.full, tag = "pred")$data
ypost <- p.res$marginals.fitted.values[pred.ind]
#Visualize PMD for the linear predictor at target locations with commands bellow
names(ypost) <- paste('y', seq_along(ypost), sep='_'); library(plyr)
xyplot(y~x | .id, ldply(ypost), panel='llines', xlab='y', ylab='Density')
#In INLA we have some functions to work with marginals distributions
apropos("marginal")
## [1] "inla.dmarginal" "inla.emarginal" "inla.hpdmarginal"
## [4] "inla.mmarginal" "inla.pmarginal" "inla.qmarginal"
## [7] "inla.rmarginal" "inla.smarginal" "inla.tmarginal"
inla.mmarginal(ypost[[1]]) ## mode
## [1] 1.697
inla.qmarginal(c(0.15, 0.7), ## quantiles
ypost[[1]])
## [1] 1.345 1.875
inla.pmarginal(inla.qmarginal(
0.3, ypost[[1]]), ypost[[1]])
## [1] 0.3

```



```{r, eval=T, echo=T}


# Let us now show how to simulate from model I3. This was the model:

# NumFledged_i ~ Poisson(mu_i)
# E(NumFledged_i)   = mu_i
# var(NumFledged_i) = mu_i
# mu_i = exp(Intercept + BreedingD.std_i + Rain.std_i + u_i)

# Where u_i ~ N(0, SIGMA)
# Recall that  u = A * w

# Simulation study spatial Poisson model.
# Set the random seed and the number of simulations
set.seed(12345)
NSim <- 1000

# Carry out the simulation
Sim <- inla.posterior.sample(n = NSim, result = I1nb)

# Now we have 1000 simulated betas and also 1000 spatial fields w 
# Use these to calculate 1000 times the expected values mu,
# and simulate count data from these using rpois

# X matrix
Xm  <- model.matrix(~BreedingD.std + Rain.std, data = OCW)

# Create space to store the information
N    <- nrow(OCW)
Ysim <- matrix(nrow = N, ncol = NSim)
mu.i <- matrix(nrow = N, ncol = NSim)
Xm   <- as.matrix(Xm)
Am   <- as.matrix(A3)

# We are calculating a Fixed part (X * beta) and also the random part (A * w)
for (i in 1: NSim){
  Betas    <- Sim[[i]]$latent[c(987, 988, 989)]
  wk       <- Sim[[i]]$latent[416:986]
  eta      <- Xm %*% Betas + Am %*% wk
  mu.i[,i] <- exp(eta)
  Ysim[,i] <- rpois(n = nrow(OCW), lambda = mu.i[,i])
}

# Now we have 1,000 simulated count data sets from the model

# And plot the results of the simulation study
Z <- matrix(nrow = max(Ysim)+1, ncol = NSim)
for (i in 1: NSim){
	zi <- table(Ysim[,i])
	I <- as.numeric(names(zi)) + 1
	Z[I,i] <- zi
}

par(mfrow = c(1,1), mar = c(5,5,2,2), cex.lab = 1.5)
Z[is.na(Z)] <- 0
Xi <- 0: max(Ysim)
AverageTable <- rowSums(Z) / NSim

AverageTable <- rowSums(Z) / NSim
apply(Z, 1, mean)
SD <- apply(Z, 1, sd)

plot(x = Xi, y = AverageTable, type = "h", lwd = 5,
	 xlab = "Simulated number of fledged chicks",
	 ylab = "Frequencies",
	 ylim = c(0, 100))
		
# Add SDs
nx <- length(SD)
NamesZ <- Xi
for (i in 1:nx){
    segments(x0 = NamesZ[i], x1 = NamesZ[i], y0 = AverageTable[i], y1 = AverageTable[i] + SD[i], lwd = 15, col = 1)
}
			
#And add the table for the observed data
Zs <- table(OCW$NumFledg)
nx <- length(Zs)
NamesZ <- as.numeric(names(Zs))
for (i in 1:nx){
    segments(x0 = NamesZ[i] + 0.2, x1 = NamesZ[i] + 0.2, y0 = 0, y1 = Zs[i], lwd = 2, col = 2)
}

# The thick black lines represent the average frequency table. The big cocoon on top of it
# is the standard deviation of the simulations. The thin red line is the frequenxy table for the observed data.

# The model does not predict enough zeros. It predicts too many ones.
#############################################		
		
```
