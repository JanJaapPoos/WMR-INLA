# Distribution map using survey data
## An example of Starry ray, using INLA
#### Authors: Geert Aarts, Niels Hintzen, Jan Jaap Poos, Ingrid Tulp

Most of the data we collect at sea is spatially and temporally correlated, such as IBTS or BTS survey data.

The statistical package INLA has the advantage over other software in R that it can combine spatial, temporal, zero-inflated, random effects etc models all in one. It therefore is very powerful and may become your one-stop-to-go package. 

##Preparing the R environment

```{r, eval=T, echo=FALSE}
rm(list=ls())
```

First, we need to set the path where the data is located and to load the relevant libraries. We need the inla package, but also a number of packages for spatial analyses and for plotting the data and the results.

```{r, eval=T, results="hide", echo=TRUE, message=FALSE, warning=FALSE}
path <- "d://WMR-INLA/data"

library(INLA); library(fields); 
library(lattice); library(latticeExtra); library(grid); library(gridExtra);
library(rgdal); library(rgeos); 
library(mapdata); library(maptools)
```


##The IBTS and BTS data

We will use the BTS and IBTS data set. In this case we downloaded the data for starry ray. These dataset contains the CPUE per length per haul can be dowloaded from datras.ices.dk. 

First we read in the IBTS dataset. The lengths are stored in mm and the CPUE is stored as number per hour. "Zero hauls" are included (hauls where no individuals are caught). For those hauls, the Lengthclass is equal to zero and CPUE number per hour is zero. The haul duration is included in the data and recorded in minutes. Because we want to use the count data rather than the CPUEs, we calculate the number per haul by multiplying the CPUE and the haulduration. In order to make sure that the counts are whole numbers we use round(). At a later stage we may include that allows downloading the data straight from the github site.

```{r, eval=T, echo=TRUE}
cpue_IBTS <- read.csv(file.path(path,"CPUE per length per haul_2017-07-12 11_18_36.csv"), stringsAsFactors = F)
cpue_IBTS$NoPerHaul <- round(cpue_IBTS$CPUE_number_per_hour*(cpue_IBTS$HaulDur/60))
cpue_IBTS <- cpue_IBTS[!(names(cpue_IBTS) %in% c("DateofCalculation", "Area", "SubArea", "DayNight","Species","Sex","DateTime","CPUE_number_per_hour"))]

cpue_IBTS$uniqueHaul <-   paste(cpue_IBTS$Year, cpue_IBTS$Survey, cpue_IBTS$Quarter, cpue_IBTS$Ship, cpue_IBTS$HaulNo, cpue_IBTS$ShootLat, cpue_IBTS$ShootLon, sep="_")

```

We have a similar but slightly different data set for BTS. However, this dataset has many more columns that we do not really need for these examples, and that we thus remove before combining the IBTS and BTS data sets. Here, the "zero hauls" have lengthclass NA and the number per haul is NA. These observations are then set to zero, so that we have the same structure as in the IBTS data set. 

```{r, eval=TRUE, echo=TRUE}
cpue_BTS  <- read.csv(file.path(path,"CPUE per length per Hour and Swept Area_2017-07-12 11_29_36.csv"), stringsAsFactors = F)

cpue_BTS <- cpue_BTS[!(names(cpue_BTS) %in% c("Country","DoorType","HaulLat","HaulLong","StatRec","DataType","Rigging","Tickler","Warplngt","TowDir","WindDir","WindSpeed","SwellDir","SwellHeight","StNo","Month","Day","TimeShot","Stratum","ICESArea","DateofCalculation", "Area", "SubArea", "DayNight","Species", "Sex","SpecVal","SubFactor","GearExp","SweepLngt","Netopening","BycSpecRecCode","StdSpecRecCode","HaulVal","CPUE_number_per_hour","CPUE_number_per_km2","SweptArea_km2","DistanceDerived", "HLNoAtLngt"))]

cpue_BTS[is.na(cpue_BTS$LngtClass),]$NoPerHaul <- 0
cpue_BTS[is.na(cpue_BTS$LngtClass),]$LngtClass <- 0

cpue_BTS$surface <- cpue_BTS$Distance * cpue_BTS$BeamWidth

cpue_BTS$uniqueHaul <-   paste(cpue_BTS$Year, cpue_BTS$Survey, cpue_BTS$Quarter, cpue_BTS$Ship, cpue_BTS$HaulNo, cpue_BTS$ShootLat, cpue_BTS$ShootLon, sep="_")

```


Next we combine the two sets into one.
```{r, eval=TRUE, echo=TRUE}
cols <- intersect(colnames(cpue_BTS), colnames(cpue_IBTS))
cpue <- rbind(cpue_BTS[,cols],cpue_IBTS[,cols])
```


Now that the IBTS and BTS data sets are combined we want to make a set where we have counts for all hauls and all lenghts. This means first making an dentifier for each unique haul (based on the information we have for all the hauls). This identifier is used to make a "trawllist" where all the information for the hauls is stored.

Once the trawllist is make we use epand.grid() to make a combination of all hauls and lenght classes. This set is merged with our original data set.
```{r, eval=TRUE, echo=TRUE}
trawllist <- cpue[!duplicated(cpue$uniqueHaul),!names( cpue) %in% c("Species","AphiaID","NoPerHaul","Sex", "LngtClass")]

#expand grid 
haulsLengths <- expand.grid(uniqueHaul=unique(cpue$uniqueHaul),LngtClass=unique(cpue$LngtClass), stringsAsFactors = F)
full_cpue <- merge(haulsLengths,cpue[names(cpue) %in% c("uniqueHaul", "LngtClass","NoPerHaul")], all.x=T)
rm(haulsLengths)
head(full_cpue)
```

After we merged all possible combinations with the data we now have NAs for those lengts and hauls where the catch is zero, and so we set those to zero. This data is subsequently merged to the trawllist so that we now have all information together. 
```{r, eval=TRUE, echo=TRUE}
full_cpue[is.na(full_cpue$NoPerHaul),]$NoPerHaul <- 0
full_cpue  <- merge(full_cpue,trawllist, all.x=T)
```

The records that have lenghts equal to zero (that indicated zero hauls in our original set ) now need to be removed because we have all the information we need (these hauls now have zero catch for the full length range observed in the survey). 

In addition, there are some observations that are highly unlikely: For instance there is a single observation of an individual of 100 cm (in 1977). This is suspicious, and we remove it from the data. 


```{r, eval=TRUE, echo=TRUE}
#now remove zero lengths
full_cpue <- full_cpue[full_cpue$LngtClass> 0,]

full_cpue <- full_cpue[full_cpue$LngtClass< 990,]
```

For our spatial correlation we will need an isomorphic coordinate system. Therefore we transform the latitudes and longitudes to UTM coordinates. these UTM coordinates are given in meters, so we divide them by 1000 to get kilometers.
```{r, eval=TRUE, echo=TRUE}
UTM <- project(cbind(full_cpue$ShootLong, full_cpue$ShootLat), "+proj=utm +zone=31U ellps=WGS84") 

full_cpue$Xkm <- UTM[,1]/1000
full_cpue$Ykm <- UTM[,2]/1000
```


The INLA code does not like special characters in some of the variable names, like the hyphen in "NS-IBTS". Therefore we rename the survey to NSIBTS. 
```{r, eval=TRUE, echo=TRUE}
#recode survey names so that there are no special characters
full_cpue[full_cpue$Survey=="NS-IBTS",]$Survey <- "NSIBTS"
```


##Subsetting and length aggregation

We still have a lot of data (`r nrow(full_cpue)`), Because these are just here as example, we will subset the data (here we only include data since 2012).    
```{r, eval=TRUE, echo=TRUE}
# make selection of  fullset.
cpue_subset <- full_cpue[full_cpue$Year >= 2012,]
```

Now we have only `r  nrow(cpue_subset)` left in cpue_subset. We start with a very simple analysis where we do not take length into account, and only analyse numbers per haul. Hence we aggregate the counts per haul.

```{r, eval=TRUE, echo=TRUE}
cpue_subset <- aggregate(NoPerHaul~uniqueHaul+Survey+  Quarter + Ship + Gear  + HaulNo + Year + HaulDur + ShootLong + ShootLat +  Xkm + Ykm +  Depth, data= cpue_subset,FUN="sum")
```

To give us a quick impression of the data we plot it in a map. The two surveys have different colors, and the size of the circles indicate the counts in each haul

```{r, eval=TRUE, echo=TRUE}
plot(cpue_subset$ShootLong,cpue_subset$ShootLat, cex=(cpue_subset$NoPerHaul)/10+0.1, col=as.numeric(as.factor(cpue_subset$Survey)))
map("world",add=T)
```

We also plot a quick histogram of the counts, to give us an impression of the statistical distribution of the data.
```{r, eval=TRUE, echo=TRUE}
hist(cpue_subset$NoPerHaul, 200)
```

The UTM coordinates combined into a Loc (location) dataset.  
```{r, eval=TRUE, echo=TRUE}
Loc <- cbind(cpue_subset$Xkm , cpue_subset$Ykm )
```

Next we need a mesh for the spatial data. Because we do not want our spatial correlations to pass landmasses (e.g. Denmark) we first make a convex hull of the data points using inla.nonconvex.hull(). This convex hull is used as a boundary for making a 2d mesh. 

```{r, eval=TRUE, echo=TRUE}

ConvHull <- inla.nonconvex.hull(Loc, convex=-0.02, resolution=90)
mesh1a    <- inla.mesh.2d(boundary = ConvHull,  max.edge=c(150))
```

As an alternative, we can also take the shapefile of the North Sea as a mesh, which makes prediction in the future easier, as we know no record can be found outside the North Sea area (give or take evolution). We first load the ICES shapefiles which were downloaded from http://gis.ices.dk/sf/. We load this shapefiles, merge layers together, transform to UTM, convert UTM to km rather than meters and finally create a mesh. 
```{r, eval=TRUE, echo=TRUE}
ICESareas <- readShapePoly(file.path(path,"ICES_Areas_20160601_dense"))
NorthSea  <- subset(ICESareas,SubArea==4)
NorthSea  <- gUnionCascaded(NorthSea)
proj4string(NorthSea) <- c("+proj=longlat")
NorthSeaUTM <- spTransform(NorthSea,CRS("+proj=utm +zone=31"))


NS.border <- inla.sp2segment(NorthSeaUTM)
NS.border$loc <- NS.border$loc/1000

mesh1b      <- inla.mesh.2d(boundary=NS.border, cutoff=3,max.edge=c(30))


```

```{r, eval=TRUE, echo=TRUE}
par(mfrow=c(1,2))
plot(mesh1a)
points(Loc, col = 1, pch = 16, cex = 0.5)
plot(mesh1b)
points(Loc, col = 1, pch = 16, cex = 0.5)
```



Once the 2d mesh is made we construct a observation/prediction weight matrix for the model. This is also called the "projector matrix".
```{r, eval=TRUE, echo=TRUE}

# 2. Define the weighting factors a_ik (also called the projector matrix).
A1      <- inla.spde.make.A(mesh  = mesh1a, loc   = Loc)
dim(A1)

# 3. Define the spde
spde  <- inla.spde2.matern(mesh1a)

w.st <- inla.spde.make.index('w', n.spde = spde$n.spde)
```

Before making the stack we need to convert all fixed effects that are factors in the INLA model.
```{r, eval=T, echo=T}
cpue_subset$fYear   <- as.factor(cpue_subset$Year)
cpue_subset$fSurvey <- as.factor(cpue_subset$Survey)
```

Note that in code below the names in the model matrix should not contain any special characters! This is why we renamd the "NS-IBTS" survey" to "NSIBTS"
```{r, eval=T, echo=T}
  
# 5. Make a stack. In this process we tell INLA at which points on the mesh we sampled the response variable and the covariates. 
Xmatrix <- model.matrix(~  fYear + fSurvey +  HaulDur  ,  data = cpue_subset)

X <- as.data.frame(Xmatrix[,-1])

names(X) <- c(gsub("[:]",".",names(X)))

N <- nrow(cpue_subset)
Stack1 <- inla.stack(
    tag  = "Fit",
    data = list(y = cpue_subset$NoPerHaul),    
    A    = list(1,1, A1),                      
    effects = list( 
       Intercept=rep(1,N),
       X=X, #Covariates
       w=w.st))                  #Spatial field
```


```{r, eval=T, results='hide', echo=T, message=FALSE, warning=FALSE}
fsp <- parse(text=c("y ~ -1 + Intercept + ",paste(c(names(X)," f(w, model = spde)"),collapse =" + ")))

#INLA:::inla.dynload.workaround() 
I1p <- inla(eval(fsp), family = "poisson", data=inla.stack.data(Stack1),
           control.compute = list(dic = TRUE, waic = TRUE),
           control.predictor = list(A = inla.stack.A(Stack1)))

I1nb <- inla(eval(fsp), family = "nbinomial", data=inla.stack.data(Stack1),
           control.compute = list(dic = TRUE, waic = TRUE, config=TRUE),
           control.predictor = list(A = inla.stack.A(Stack1)))
```

```{r, eval=T, echo=T}

summary(I1p)

summary(I1nb)
```

```{r, eval=T, echo=T}

idx <- inla.stack.index(Stack1, tag= 'Fit')$data
quantile(I1nb$summary.fitted.values[idx,"mean"])
quantile(cpue_subset$NoPerHaul)

hist(I1nb$summary.fitted.values[idx,"mean"],200, xlim=c(0,50))
hist(cpue_subset$NoPerHaul,200, xlim=c(0,50))

```


```{r, eval=T, echo=T, fig.width=8, fig.height=4}
wproj <- inla.mesh.projector(mesh1a, xlim = range(Loc[,1]), ylim = range(Loc[,2])) 

wm.pm100100  <- inla.mesh.project(wproj, I1nb$summary.random$w$mean)
wsd.pm100100 <- inla.mesh.project(wproj, I1nb$summary.random$w$sd)
    
grid     <- expand.grid(x = wproj$x, y = wproj$y)
grid$zm  <- as.vector(wm.pm100100)   
grid$zsd <- as.vector(wsd.pm100100)   

wld <- map('world', xlim=c(-5,15), ylim=c(47,62),plot=FALSE)
wld <- data.frame(lon=wld$x, lat=wld$y)

UTMmap <- project(cbind(wld$lon, wld$lat), "+proj=utm +zone=31U ellps=WGS84")
UTMmapFinal <- data.frame("xm"  =UTMmap[,1]/1e3, "ym" = UTMmap[,2]/1e3)

p1 <- levelplot(zm ~ x * y  ,
          data = grid, 
          scales = list(draw = TRUE),
          xlab = list("Easting", cex = 1),
          ylab = list("Northing", cex = 1),
          main = list("Posterior mean spatial random fields", cex = 1),
          col.regions=tim.colors(25, alpha = 1),
          panel=function(x, y, z, subscripts,...){
            panel.levelplot(x, y, z, subscripts,...)
            grid.points(x = cpue_subset$Xkm,
                        y = cpue_subset$Ykm, 
                        pch = 1,
                        size = unit(cpue_subset$NoPerHaul/10, "char"))  
          }) +  xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black')

p2 <- levelplot(zsd ~ x * y,
          data = grid, 
          scales = list(draw = TRUE),
          xlab = list("Easting", cex = 1),
          ylab = list("Northing", cex = 1),
          main = list("Posterior sd spatial random fields", cex = 1),
          col.regions=tim.colors(25, alpha = 1),
          panel=function(x, y, z, subscripts,...){
            panel.levelplot(x, y, z, subscripts,...)
            grid.points(x = cpue_subset$Xkm,
                        y = cpue_subset$Ykm, 
                        pch = 1,
                        size = unit(cpue_subset$NoPerHaul/10, "char"))  
          }) +  xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black')

grid.arrange(p1,p2, ncol=2)
  
```

```{r, eval=FALSE, echo=T}

tcoo <- rbind(c(0.3, 0.3), c(0.5, 0.5), c(0.7, 0.7))
inla.nonconvex.hull
dim(Ap <- inla.spde.make.A(mesh = mesh1, loc = tcoo))
## [1] 3 505
x0 <- c(0.5, 0.5, 0.5)
#To do a fully Bayesian analysis, we include the target locations on the estimation process by
#assigning NA for the response at these locations. Defining the prediction stack
stk.pred <- inla.stack(tag='pred', A=list(Ap, 1), data=list(y=NA), ## response as NA
effects=list(s=1:spde$n.spde, data.frame(x=x0, b0=1)))
#Fit the model again with the full stack

stk.full <- inla.stack(stk.e, stk.pred)
p.res <- inla(formula, data=inla.stack.data(stk.full), ## full stack
control.predictor=list(compute=TRUE, ## compute the predictor
A=inla.stack.A(stk.full))) ## using full stack data
#Get the prediction data index and collect the PMD to work with
pred.ind <- inla.stack.index(stk.full, tag = "pred")$data
ypost <- p.res$marginals.fitted.values[pred.ind]
#Visualize PMD for the linear predictor at target locations with commands bellow
names(ypost) <- paste('y', seq_along(ypost), sep='_'); library(plyr)
xyplot(y~x | .id, ldply(ypost), panel='llines', xlab='y', ylab='Density')
#In INLA we have some functions to work with marginals distributions
apropos("marginal")
## [1] "inla.dmarginal" "inla.emarginal" "inla.hpdmarginal"
## [4] "inla.mmarginal" "inla.pmarginal" "inla.qmarginal"
## [7] "inla.rmarginal" "inla.smarginal" "inla.tmarginal"
inla.mmarginal(ypost[[1]]) ## mode
## [1] 1.697
inla.qmarginal(c(0.15, 0.7), ## quantiles
ypost[[1]])
## [1] 1.345 1.875
inla.pmarginal(inla.qmarginal(
0.3, ypost[[1]]), ypost[[1]])
## [1] 0.3

```


##Including length in the analysis

Owing to ontogenetic niche shifts we expect that the spatial distribution of small indivduals is different from the spatial distribution of large indivuals. Hence we want to include a length component in the spatial distribution of the counts.

First we need to make a new subset of the data that includes the length information (and leaving out the aggregation step in our earlier analysis).

```{r, eval=TRUE, echo=TRUE}
# make selection of  fullset.
cpue_subset <- full_cpue[full_cpue$Year >= 2012,]
```

Let's make 5 cm classes instead of 1 cm classes. This reduces the number of observations. We use round to achieve this. Remember that the units of the length measurements is mm. 

Once we have 5 cm lenght classes, we need aggregate() to sum the numbers per haul within our new length bins. Becaus we need the other info in the data set as well we include all variables in the aggregate. 

```{r, eval=TRUE, echo=TRUE}
cpue_subset$LngtClass <- round(cpue_subset$LngtClass/50)*50

cpue_subset <- aggregate(NoPerHaul~ LngtClass + uniqueHaul+Survey+  Quarter + Ship + Gear  + HaulNo + Year + HaulDur + ShootLong + ShootLat + Xkm + Ykm +  Depth, data= cpue_subset,FUN="sum")
```

Because we now have combinations for all hauls and several length classes, the subset of data (since 2012) is still very big. It has`r  nrow(cpue_subset)` observations. In order to reduce the number of observaions used in the INLA model, we take all non-zero observations but we subsample the zero observations. The remaining zero observations willneed to receive a higher weight in the final model. Note that this is not yet implemented.  
```{r, eval=TRUE, echo=TRUE}
cpue_subset_temp  <- cpue_subset[cpue_subset$NoPerHaul > 0,]
cpue_subset_temp  <-rbind(cpue_subset_temp, cpue_subset[sample(which(cpue_subset$NoPerHaul==0),10000),])

cpue_subset <- cpue_subset_temp
```


As before, The UTM coordinates of the observations are combined into a Loc (location) dataset. That dataset is used to create a mesh in the next step.  
```{r, eval=TRUE, echo=TRUE}
Loc <- cbind(cpue_subset$Xkm , cpue_subset$Ykm )
```

Next we need a mesh for the spatial data. Because we do not want our spatial correlations to pass landmasses (e.g. Denmark) we first make a convex hull of the data points using inla.nonconvex.hull(). This convex hull is used as a boundary for making a 2d mesh. 

```{r, eval=TRUE, echo=TRUE}

ConvHull <- inla.nonconvex.hull(Loc, convex=-0.02, resolution=90)
mesh2a    <- inla.mesh.2d(boundary = ConvHull,  max.edge=c(150))
plot(mesh2a)
points(Loc, col = 1, pch = 16, cex = 0.5)
```

Next we inspect the length range in the data by making a histogram of length observations. This histogram can be used to define the locations ofa number of "knots" along the length range that we will later use for our analysis. More knots means a longer computing time (but a higher degree of flexibility in the length component of the spatial correlation). 


```{r, eval=T, echo=T}
hist(cpue_subset$LngtClass)
Knots <- seq(from = 50, to = 850, by = 200)
Knots 
```

Using the knots as locations, we make a 1 dimensional mesh.   
```{r, eval=T, echo=T}
  
# One-dimensional mesh for length class. See the time series module
mesh.t <- inla.mesh.1d(loc = Knots)
mesh.t$loc
mesh.t$n   #We have 10 random fields
```

Now that we have a 1 dimensional mesh for the lengths, we use it  to construct a observation/prediction weight matrix ("projector matrix") based on the spatial mesh that we already created earlier (mesh1) and our new mesh for the lengths. The lengths are used in the "group model". The new projector matrix is names A2 to distinguish it from the projector matrix of the previous model.

```{r, eval=T, echo=T}
# 2. Define the weighting factors a_ik (projector matrix).
NGroups <- length(Knots) # = 7  
A2      <- inla.spde.make.A(mesh  = mesh2a,
                            loc   = Loc,
                            group = cpue_subset$LngtClass, 
                       group.mesh = mesh.t)
dim(A2) 

# 3. Define the spde
spde  <- inla.spde2.matern(mesh2a)
```

We need to make an inla.spde model object for a Matern model, but we still have that available from the model without size structure. That object was named "spde". We use it to make a list of named index vectors for the SPDE model. Note that the command for making the list of index vectors now includes an argument for the groups. 

```{r, eval=T, echo=T}
w.st <- inla.spde.make.index('w', 
                             n.spde = spde$n.spde, 
                             n.group = NGroups)
```


Before making the stack we need to convert all fixed effects that are factors in the INLA model.
```{r, eval=T, echo=T}
cpue_subset$fYear   <- as.factor(cpue_subset$Year)
cpue_subset$fSurvey <- as.factor(cpue_subset$Survey)
```

Next we make a new stack. For this we need a model matrix. Although the fixed effects are the same as in the previous model, we still need to make a new model matrix because the data now include the length structure. 

```{r, eval=T, echo=T}
# 5. Make a stack. 
Xmatrix <- model.matrix(~  fYear + fSurvey +  HaulDur,  data = cpue_subset)

head(Xmatrix)
X <- as.data.frame(Xmatrix[,-1])
names(X) <- c(gsub("[:]",".",names(X)))
head(X)

N <- nrow(cpue_subset)
Stack2 <- inla.stack(
  tag  = "Fit",
  data = list(y = cpue_subset$NoPerHaul),    
  A    = list(1,1, A2),                      
  effects = list(  
    Intercept = rep(1, N),       
    X = as.data.frame(X), # Covariates
    w.st))                # Spatial-temp field  

```

```{r, eval=T, echo=T}
fsp <- parse(text=c("y ~ -1 + Intercept + ",
                    paste(c(names(X)," f(w, model = spde, group =       w.group, control.group = list(model = 'ar1'))"),collapse =" + ")))
```

```{r, eval=T, echo=T}
#INLA:::inla.dynload.workaround() 
I2nb <- inla(eval(fsp), family = "nbinomial",
           data=inla.stack.data(Stack2),
           control.compute = list(dic = TRUE, waic = TRUE),
           control.predictor = list(A = inla.stack.A(Stack2)))
```

```{r, eval=T, echo=T}
summary(I2nb)
```

The "UTMmap" object for creating the maps we still have.

```{r, eval=T, echo=T}
w <- I2nb$summary.random$w$mean
# length of w is mesh$n * NGroups

wproj <- inla.mesh.projector(mesh2a, xlim = range(Loc[,1]), ylim = range(Loc[,2])) 

grid <- expand.grid(IDX=1:NGroups, x = wproj$x, y = wproj$y,zm=NA)

for (i in 1: NGroups){
    w.pm100100 <- inla.mesh.project(wproj,w[w.st$w.group==i])
    grid[grid$IDX==i,]$zm <- as.vector(w.pm100100)  }
```

Next we print the grid, which is now estimated at each knot.
```{r, eval=T, echo=T}
print(levelplot(zm ~ x * y |IDX,
          data = grid,
          scales = list(draw = TRUE),
          xlab = list("Easting", cex = 1),
          ylab = list("Northing", cex = 1),
          main = list("Posterior mean spatial random fields", cex = 1),
          col.regions=tim.colors(25, alpha = 1),
          panel=function(x, y, z, subscripts,...){
            panel.levelplot(x, y, z, subscripts,...)
            grid.points(x = cpue_subset$Xkm,
                        y = cpue_subset$Ykm, 
                        pch = 1,
                        size = unit(cpue_subset$NoPerHaul/10, "char"))  
          }) +  xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black'))
```



##Including length in the survey catchability

```{r, eval=T, echo=T}
# 5. Make a stack. 
Xmatrix <- model.matrix(~  fYear + fSurvey + HaulDur,  data = cpue_subset)

head(Xmatrix)
X <- as.data.frame(Xmatrix[,-1])
names(X) <- c(gsub("[:]",".",names(X)))
head(X)

N <- nrow(cpue_subset)
Stack3 <- inla.stack(
  tag  = "Fit",
  data = list(y = cpue_subset$NoPerHaul),    
  A    = list(1,1, A2),                      
  effects = list(  
    Intercept = rep(1, N),       
    X = as.data.frame(X), # Covariates
    w.st))                # Spatial-temp field  

```

```{r, eval=FALSE, echo=T}
fsp <- parse(text=c("y ~ -1 + Intercept + ",
                    paste(c(names(X)," f(w, model = spde, group =       w.group, control.group = list(model = 'ar1'))", "f(LngtClass, group=fSurvey,'rw2')",collapse =" + ")))
```

```{r, eval=FALSE, echo=T}
#INLA:::inla.dynload.workaround() 
I3nb <- inla(eval(fsp), family = "nbinomial",
           data=inla.stack.data(Stack3),
           control.compute = list(dic = TRUE, waic = TRUE),
           control.predictor = list(A = inla.stack.A(Stack3)))
```

```{r, eval=FALSE, echo=T}
summary(I3nb)
```

## Using swept area estimates
Reread IBTS data 

```{r, eval=T, echo=TRUE}
cpue_IBTS <- read.csv(file.path(path,"CPUE per length per haul_2017-07-12 11_18_36.csv"), stringsAsFactors = F)
cpue_IBTS$NoPerHaul <- round(cpue_IBTS$CPUE_number_per_hour*(cpue_IBTS$HaulDur/60))
cpue_IBTS <- cpue_IBTS[!(names(cpue_IBTS) %in% c("DateofCalculation", "Area", "SubArea", "DayNight","Species","Sex","DateTime","CPUE_number_per_hour"))]

cpue_IBTS$uniqueHaul <-   paste(cpue_IBTS$Year, cpue_IBTS$Survey, cpue_IBTS$Quarter, cpue_IBTS$Ship, cpue_IBTS$HaulNo, cpue_IBTS$ShootLat, cpue_IBTS$ShootLon, sep="_")

```


File contains HH records in exchange files for IBTS. NA for door spread and distance are coded as -9, so make NAs of those. 
```{r, eval=T, echo=T}
HH_IBTS <- read.csv(file.path(path,"Exchange Data_2017-07-14 09_26_25.csv"), stringsAsFactors = F)

HH_IBTS$uniqueHaul <-  paste(HH_IBTS$Year, HH_IBTS$Survey, HH_IBTS$Quarter, HH_IBTS$Ship, HH_IBTS$HaulNo, HH_IBTS$ShootLat, HH_IBTS$ShootLon, sep="_")

HH_IBTS[HH_IBTS$DoorSpread == -9,]$DoorSpread <- NA
HH_IBTS[HH_IBTS$Distance == -9,]$Distance <- NA
HH_IBTS[HH_IBTS$Netopening == -9,]$Netopening <- NA

HH_IBTS$surface <- HH_IBTS$Distance * HH_IBTS$Netopening
```

Select only relevant variables, and remove all hauls longer than 90 minutes
```{r, eval=T, echo=T}
HH_IBTS <- HH_IBTS[names(HH_IBTS) %in% c("uniqueHaul", "surface", "HaulDur","Doorspread","Ditance","Netopening","Warplngt")]
HH_IBTS <- HH_IBTS[HH_IBTS$HaulDur<=90,]
```

Plot surface against duration hoping to find corr
```{r, eval=T, echo=T}
plot(x=HH_IBTS$HaulDur,y=HH_IBTS$surface)
summary(lm(surface~HaulDur, data=HH_IBTS))
```


Merge haulinfo to full cpue set
```{r, eval=T, echo=T}
cpue_IBTS <- merge(cpue_IBTS, HH_IBTS, all.x=T, all.y=F)
```



```{r, eval=TRUE, echo=TRUE}
cpue_BTS  <- read.csv(file.path(path,"CPUE per length per Hour and Swept Area_2017-07-12 11_29_36.csv"), stringsAsFactors = F)
cpue_BTS <- cpue_BTS[!(names(cpue_BTS) %in% c("Country","DoorType","HaulLat","HaulLong","StatRec","DataType","Rigging","Tickler","Warplngt","TowDir","WindDir","WindSpeed","SwellDir","SwellHeight","StNo","Month","Day","TimeShot","Stratum","ICESArea","DateofCalculation", "Area", "SubArea", "DayNight","Species", "Sex","SpecVal","SubFactor","GearExp","SweepLngt","Netopening","BycSpecRecCode","StdSpecRecCode","HaulVal","CPUE_number_per_hour","CPUE_number_per_km2","SweptArea_km2","DistanceDerived", "HLNoAtLngt"))]
surface
cpue_BTS[is.na(cpue_BTS$LngtClass),]$NoPerHaul <- 0
cpue_BTS[is.na(cpue_BTS$LngtClass),]$LngtClass <- 0

cpue_BTS$surface <- cpue_BTS$Distance * cpue_BTS$BeamWidth

cpue_BTS$uniqueHaul <-   paste(cpue_BTS$Year, cpue_BTS$Survey, cpue_BTS$Quarter, cpue_BTS$Ship, cpue_BTS$HaulNo, cpue_BTS$ShootLat, cpue_BTS$ShootLon, sep="_")

```

Next we combine the two sets into one.
```{r, eval=TRUE, echo=TRUE}
cols <- intersect(colnames(cpue_BTS), colnames(cpue_IBTS))
cpue <- rbind(cpue_BTS[,cols],cpue_IBTS[,cols])
```


Now that the IBTS and BTS data sets are combined we want to make a set where we have counts for all hauls and all lenghts. This means first making an dentifier for each unique haul (based on the information we have for all the hauls). This identifier is used to make a "trawllist" where all the information for the hauls is stored.

Once the trawllist is make we use epand.grid() to make a combination of all hauls and lenght classes. This set is merged with our original data set.
```{r, eval=TRUE, echo=TRUE}
trawllist <- cpue[!duplicated(cpue$uniqueHaul),!names( cpue) %in% c("Species","AphiaID","NoPerHaul","Sex", "LngtClass")]

#expand grid 
haulsLengths <- expand.grid(uniqueHaul=unique(cpue$uniqueHaul),LngtClass=unique(cpue$LngtClass), stringsAsFactors = F)
full_cpue <- merge(haulsLengths,cpue[names(cpue) %in% c("uniqueHaul", "LngtClass","NoPerHaul")], all.x=T)
rm(haulsLengths)
head(full_cpue)
```

After we merged all possible combinations with the data we now have NAs for those lengts and hauls where the catch is zero, and so we set those to zero. This data is subsequently merged to the trawllist so that we now have all information together. 
```{r, eval=TRUE, echo=TRUE}
full_cpue[is.na(full_cpue$NoPerHaul),]$NoPerHaul <- 0
full_cpue  <- merge(full_cpue,trawllist, all.x=T)
```

The records that have lenghts equal to zero (that indicated zero hauls in our original set ) now need to be removed because we have all the information we need (these hauls now have zero catch for the full length range observed in the survey). 

In addition, there are some observations that are highly unlikely: For instance there is a single observation of an individual of 100 cm (in 1977). This is suspicious, and we remove it from the data. 


```{r, eval=TRUE, echo=TRUE}
#now remove zero lengths
full_cpue <- full_cpue[full_cpue$LngtClass> 0,]

full_cpue <- full_cpue[full_cpue$LngtClass< 990,]
```

For our spatial correlation we will need an isomorphic coordinate system. Therefore we transform the latitudes and longitudes to UTM coordinates. these UTM coordinates are given in meters, so we divide them by 1000 to get kilometers.
```{r, eval=TRUE, echo=TRUE}
UTM <- project(cbind(full_cpue$ShootLong, full_cpue$ShootLat), "+proj=utm +zone=31U ellps=WGS84") 

full_cpue$Xkm <- UTM[,1]/1000
full_cpue$Ykm <- UTM[,2]/1000
```


The INLA code does not like special characters in some of the variable names, like the hyphen in "NS-IBTS". Therefore we rename the survey to NSIBTS. 
```{r, eval=TRUE, echo=TRUE}
#recode survey names so that there are no special characters
full_cpue[full_cpue$Survey=="NS-IBTS",]$Survey <- "NSIBTS"
```


```{r, eval=TRUE, echo=TRUE}
# make selection of  fullset.
cpue_subset <- full_cpue[full_cpue$Year >= 2013,]
```

Do actual INLA for counts per km2 :)


Next we want to simulate 1000 realizations and integrate over surface (per year) so that we get a population level estimate.  
```{r, eval=FALSE, echo=T}
# Simulate from model I1nb

# Simulation study spatial Negbin model.
# First set random seed and the number of simulations
set.seed(12345)
NSim <- 1000

# Carry out the simulation
Sim <- inla.posterior.sample(n = NSim, result = I1nb)

# Now we have 1000 simulated betas and also 1000 spatial fields w 
# Use these to calculate 1000 times the expected values mu,
# and simulate count data from these using rpois

# X matrix
#Xm      <- model.matrix(~BreedingD.std + Rain.std, data = cpue_subset)
Xm <- model.matrix(~  fYear + fSurvey +  HaulDur  ,  data = cpue_subset)

# Create space to store the information
N    <- nrow(cpue_subset)
Ysim <- matrix(nrow = N, ncol = NSim)
mu.i <- matrix(nrow = N, ncol = NSim)
Xm   <- as.matrix(Xm)
Am   <- as.matrix(A3)

# We are calculating a Fixed part (X * beta) and also the random part (A * w)
for (i in 1: NSim){
  Betas    <- Sim[[i]]$latent[c(987, 988, 989)]
  wk       <- Sim[[i]]$latent[416:986]
  eta      <- Xm %*% Betas + Am %*% wk
  mu.i[,i] <- exp(eta)
  Ysim[,i] <- rpois(n = nrow(OCW), lambda = mu.i[,i])
}

# Now we have 1,000 simulated count data sets from the model

# And plot the results of the simulation study
Z <- matrix(nrow = max(Ysim)+1, ncol = NSim)
for (i in 1: NSim){
	zi <- table(Ysim[,i])
	I <- as.numeric(names(zi)) + 1
	Z[I,i] <- zi
}


#############################################		
		
```