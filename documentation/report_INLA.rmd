
# Distribution map using survey data
## An example of Starry ray in the North Sea, using INLA
#### Authors: Geert Aarts, Niels Hintzen, Harriet van Overzee, Jurgen Batsleer, Jan Jaap Poos, Ingrid Tulp

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 120)
rm(list=ls())
```

Set correct path for data, load libraries and load the full_cpue data set 

First, we need to set the path where the data is located and to load the relevant libraries. The inla package can be installed using install.packages("INLA", repos="https://inla.r-inla-download.org/R/stable"). We need the inla package, but also a number of packages for spatial analyses ('rgeos' and 'rgdal')  and for plotting the data and the results ('lattice', 'latticeExtra', 'grid','gridExtra','mapdata', and 'maptools').

```{r, eval=T, results="hide", echo=TRUE, message=FALSE, warning=FALSE}
path <- "~/WMR-INLA/data"
#path <- "d://WMR-INLA/data" 

library(INLA); library(fields); 
library(mgcv)
library(lattice); library(latticeExtra); library(grid); library(gridExtra);
library(rgdal); library(rgeos); 
library(mapdata); library(maptools)

load(file.path(path,"survey_data.Rdata"))
```

### Subsetting and length aggregation

We still have a lot of data (`r nrow(full_cpue)`), Because these are just here as example, we will subset the data, and start our analysis from a fiven year. That year is stored in startyr.    
```{r, eval=TRUE, echo=TRUE}
startyr <- 2005
cpue_subset <- full_cpue[full_cpue$Year >= startyr,]
```

In this example we thus start from `r startyr`.  The earlier we start, the more information we will obtain from the analysis on the annual changes in the count data. However, including more data also means we require more memory to store the model, and wait longer for the model to run. If too much data is used in the inla model, so that more computer memory is required than you have available, the model will crash. Now we have `r  nrow(cpue_subset)` rows left in cpue_subset. 

```{r, eval=T, echo=TRUE}
cpue_subset <- aggregate(NoPerHaul ~ haulCode + Survey +  Quarter + Ship + Gear  + HaulNo + Year + HaulDur + surface + ShootLong + ShootLat +  Xkm + Ykm +  Depth, data= cpue_subset,FUN="sum")
```
After the aggregation (now that the length information is removed) we have `r  nrow(cpue_subset)` rows in the data set. To give us a quick impression of the data we plot it in a map. The two surveys have different colors, black being the BTS hauls, red being the IBTS hauls in quarter 1 and blue being the IBTS hauls in quarter 3. The size of the circles indicate the counts in each haul.
```{r, eval=T, echo=TRUE, dpi=600}
par(mfrow=c(1,2))
plot(cpue_subset$ShootLong,cpue_subset$ShootLat, 
     cex=0.1, 
     col=c('black', 'red','blue')[as.factor(paste(cpue_subset$Survey, cpue_subset$Quarter))],
     xlab="Longitude", ylab="Lattitude", main="Location of all hauls" )
map("world",add=T)

plot(cpue_subset$ShootLong,cpue_subset$ShootLat, 
     cex=log(cpue_subset$NoPerHaul)/5, 
     col=c('black', 'red','blue')[as.factor(paste(cpue_subset$Survey, cpue_subset$Quarter))],
     xlab="Longitude", ylab="Lattitude",  main="Counts per hauls" )
map("world",add=T)
```
Clearly the IBTS hauls cover a larger part of the North Sea, but the large catches are located in the same areas. 

### The count data

We plot a quick histogram of the counts to give us an impression of the statistical distribution of the data.

```{r, eval=T, echo=TRUE, dpi=600}
par(mfrow=c(1,3))
hist(cpue_subset[cpue_subset$Survey == "BTS",]$NoPerHaul, 200, xlab= "Number per haul", main="Counts per haul BTS")
hist(cpue_subset[cpue_subset$Survey == "NSIBTS" &cpue_subset$Quarter == 1,]$NoPerHaul, 200, xlab= "Number per haul", main="Counts per haul IBTS q1")
hist(cpue_subset[cpue_subset$Survey == "NSIBTS" &cpue_subset$Quarter == 3,]$NoPerHaul, 200, xlab= "Number per haul", main="Counts per haul IBTS q3")

```

The count data is clearly quite skewed, with lots of zeros and small values, and very few large counts. Because of this distribution and because  these are counts, a Poisson and a negative binomial distribution will be used in the inla model. We will also test if zero-inflation provides better fits to the data.  

## Making a spatial mesh for the data

The UTM coordinates of the data set are combined into a Loc (location) dataset. That data set will be used later to generate spatial meshes for the data.   
```{r, eval=T, echo=TRUE}
Loc <- cbind(cpue_subset$Xkm , cpue_subset$Ykm )
```

Next we need a mesh for the spatial data. Because we do not want our spatial correlations to pass landmasses (e.g. Denmark) we first make a nonconvex hull of the data points using inla.nonconvex.hull(). This convex hull is used as a boundary for making a 2d mesh. The generation of the actual mesh is done using inla.mesh.2d(). That function takes several arguments, incuding "cutoff" and "max.edge". These arguments specify how fine the final mesh will be. Finer meshes will be able to capture smaller scale spatial correlations, but require more comuting time in the inla model.

```{r, eval=T, echo=TRUE}
ConvHull <- inla.nonconvex.hull(Loc, convex=-0.02, resolution=90)
mesh1a   <- inla.mesh.2d(boundary = ConvHull,  max.edge=c(40))
```

As an alternative to using the nonconvex hull function to generate a boundary,, we can also take the shapefile of the North Seaas the boundary of the mesh, which makes prediction in the future easier, as we know no record can be found outside the North Sea area (give or take evolution). We first load the ICES shapefiles which were downloaded from http://gis.ices.dk/sf/. We load this shapefiles, merge layers together, transform to UTM, convert UTM to km rather than meters and finally create a mesh. 
```{r, eval=T, echo=TRUE}
ICESareas <- readShapePoly(file.path(path,"ICES_Areas_20160601_dense"))
NorthSea  <- subset(ICESareas,SubArea==4)
NorthSea  <- gUnionCascaded(NorthSea)
proj4string(NorthSea) <- c("+proj=longlat")
NorthSeaUTM <- spTransform(NorthSea,CRS("+proj=utm +zone=31"))

NS.border <- inla.sp2segment(NorthSeaUTM)
NS.border$loc <- NS.border$loc/1000

mesh1b      <- inla.mesh.2d(boundary=NS.border, cutoff=3,max.edge=c(40))
```

The meshes can be plotted using the plot() function on the mesh object. Once the mesh is plotted, the locations of the samples (stored in the Loc object) can be overlayed using points(). Below, the two meshes are plotted side-by-side.

```{r, eval=T, echo=TRUE, dpi=600}
par(mfrow=c(1,2))
plot(mesh1a)
points(Loc, col = 1, pch = 16, cex = 0.3)
plot(mesh1b)
points(Loc, col = 1, pch = 16, cex = 0.3)
```

## Making the projector matrix and the spde 

Once the 2d mesh is made we construct a observation/prediction weight matrix for the model. This is also called the "projector matrix".
```{r, eval=T, echo=TRUE}
# 2. Define the weighting factors a_ik (also called the projector matrix).
A1      <- inla.spde.make.A(mesh  = mesh1a, loc   = Loc)
dim(A1)
```
The first dimension of the projector matrix has the size of the number of observations (here `r dim(A1)[1]`), and the second dimension of the projector matrix is the number of nodes in the mesh (here `r dim(A1)[2]`).
```{r, eval=T, echo=TRUE}
# 3. Define the spde
spde  <- inla.spde2.matern(mesh1a)

w.st <- inla.spde.make.index('w', n.spde = spde$n.spde)
```

## Making the stack

The stack allows INLA to build models with complex linear predictors. Here have a SPDE model combined with covariate fixed effects and an intercept at n hauls.

Before making the stack we need to convert all fixed effects that are factors in the INLA model.
```{r, eval=T, echo=T}
cpue_subset$fYear   <- as.factor(cpue_subset$Year)
cpue_subset$fSurvey <- as.factor(cpue_subset$Survey)
```

Because of the link function of the Poisson and negative binomial that we will be using we need to log-transform the surfaces and haul durations to get a linear response later. 
```{r, eval=T, echo=T}
cpue_subset$lsurface   <- log(cpue_subset$surface)
cpue_subset$lhauldur   <- log(cpue_subset$HaulDur)
```

Note that in code below the names in the model matrix should not contain any special characters! This is why we renamed the "NS-IBTS" survey to "NSIBTS"
```{r, eval=T, echo=T}
# 5. Make a stack. In this process we tell INLA at which points on the mesh we sampled the response variable and the covariates. 
Xmatrix <- model.matrix(~ fYear + fSurvey +  lhauldur,  data = cpue_subset)
head(Xmatrix)
```

This Xmatrix contains the model matrix with the fixed effects, including the intercept (The column for the intercept is named "(Intercept)", and it is 1 for all observations). However, in the next step the intercept is removed from the model matrix. The intercept is then included when making the stack, and named "Intercept" (without brackets).

```{r, eval=T, echo=T}
X <- as.data.frame(Xmatrix[,-1])
names(X) <- c(gsub("[:]",".",names(X)))
head(X)

N <- nrow(cpue_subset)
Stack1 <- inla.stack(
    tag  = "Fit",
    data = list(y = cpue_subset$NoPerHaul),    
    A    = list(1,1, A1),                      
    effects = list( 
       Intercept=rep(1,N),
       X=X, #Covariates
       w=w.st))                  #Spatial field
```

## Making the model formula and running the INLA model

The model formula used in the inla model is generated from the names of the model matrix, combined with the intercept term and the spatial correlation model ("f(w, model=spde)").

Subsequently, two inla models are run, one assuming that the data are Poisson distributed, and another model assuming that the data are negative binomial distributed.

```{r, eval=T, results='hide', echo=T, message=FALSE, warning=FALSE}
fsp <- parse(text=c("y ~ -1 + Intercept + ",paste(c(names(X)," f(w, model = spde)"),collapse =" + ")))

INLA:::inla.dynload.workaround() 
I1p <- inla(eval(fsp), family = "poisson", data=inla.stack.data(Stack1),
           control.compute = list(dic = TRUE, waic = TRUE),
           control.predictor = list(A = inla.stack.A(Stack1)))

I1nb <- inla(eval(fsp), family = "nbinomial", data=inla.stack.data(Stack1),
           control.compute = list(dic = TRUE, waic = TRUE, config=TRUE),
           control.predictor = list(A = inla.stack.A(Stack1)))

I1zip <- inla(eval(fsp), family = "zeroinflatedpoisson1", data=inla.stack.data(Stack1),
           control.compute = list(dic = TRUE, waic = TRUE),
           control.predictor = list(A = inla.stack.A(Stack1)))

I1zinb <- inla(eval(fsp), family = "zeroinflatednbinomial1", data=inla.stack.data(Stack1),
           control.compute = list(dic = TRUE, waic = TRUE, config=TRUE),
           control.predictor = list(A = inla.stack.A(Stack1)))
```
Once the INLA models are run, a summary can be printed for the two models. This summary contains much of the relevant information for the models. Below is the example for the Poisson model. The summary also  contains the WAIC criterion. WAIC works just like AIC in that the model with the lowest WAIC should be selected.

```{r, eval=T, echo=T}
summary(I1p)
summary(I1p)$waic$waic
```

The Poisson model this has a WAIC value of `r summary(I1p)$waic$waic`. We can make a simple table to compare the WAIC values of the different models.

```{r, eval=T, echo=T}
dic2  <- c(I1p$dic$dic, I1nb$dic$dic, I1zip$dic$dic, I1zinb$dic$dic)
waic2 <- c(I1p$waic$waic, I1nb$waic$waic, I1zip$waic$waic, I1zinb$waic$waic)
Z     <- cbind(dic2, waic2)
rownames(Z) <- c("Poisson model",  "Negative binomial model", "Zero Inflated Poisson model",  "Zero Inflated Negative binomial model" )
Z
```

The negative binomial model has the lowest DIC and WAIC. Let's see the summary of this model.  
```{r, eval=T, echo=T}
summary(I1nb)
```
Plot the histograms of observations and fits for all models. 

```{r, eval=T, echo=T, dpi=600}
idx <- inla.stack.index(Stack1, tag= 'Fit')$data

par(mfrow=c(2,2))
pos <- barplot(log(table(cpue_subset$NoPerHaul)),names.arg=as.numeric(names(table(cpue_subset$NoPerHaul))),ylim=c(0,log(max(table(cpue_subset$NoPerHaul),na.rm=T))*1.3),main="Poisson model")
breaks <- c(as.numeric(names(table(cpue_subset$NoPerHaul))),1000)
points(y=log(table(cut(I1p$summary.fitted.values[idx,"mean"],breaks=breaks))),x=pos[,1],col=2,pch=19)

pos <- barplot(log(table(cpue_subset$NoPerHaul)),names.arg=as.numeric(names(table(cpue_subset$NoPerHaul))),ylim=c(0,log(max(table(cpue_subset$NoPerHaul),na.rm=T))*1.3),main="Negative binomial model")
breaks <- c(as.numeric(names(table(cpue_subset$NoPerHaul))),1000)
points(y=log(table(cut(I1nb$summary.fitted.values[idx,"mean"],breaks=breaks))),x=pos[,1],col=2,pch=19)

pos <- barplot(log(table(cpue_subset$NoPerHaul)),names.arg=as.numeric(names(table(cpue_subset$NoPerHaul))),ylim=c(0,log(max(table(cpue_subset$NoPerHaul),na.rm=T))*1.3),main="ZIP model")
breaks <- c(as.numeric(names(table(cpue_subset$NoPerHaul))),1000)
points(y=log(table(cut(I1zip$summary.fitted.values[idx,"mean"],breaks=breaks))),x=pos[,1],col=2,pch=19)

pos <- barplot(log(table(cpue_subset$NoPerHaul)),names.arg=as.numeric(names(table(cpue_subset$NoPerHaul))),ylim=c(0,log(max(table(cpue_subset$NoPerHaul),na.rm=T))*1.3),main="ZINB model")
breaks <- c(as.numeric(names(table(cpue_subset$NoPerHaul))),1000)
points(y=log(table(cut(I1zinb$summary.fitted.values[idx,"mean"],breaks=breaks))),x=pos[,1],col=2,pch=19)
```

```{r, eval=T, echo=T,  dpi=600}
# Spatial info
SpatField.w <- inla.spde2.result(inla = I1nb, name = "w", spde = spde, do.transfer = TRUE)

Kappa <- inla.emarginal(function(x) x, 
                        SpatField.w$marginals.kappa[[1]] )

Sigma_u <- inla.emarginal(function(x) sqrt(x), 
                        SpatField.w$marginals.variance.nominal[[1]] )

range <- inla.emarginal(function(x) x, 
                        SpatField.w$marginals.range.nominal[[1]] )

Kappa; Sigma_u; range       #Distance at which the correlation diminishes

# Show correlation structure
LocMesh <- mesh1a$loc[,1:2]

# And then we calculate the distance between each vertex.
D <- as.matrix(dist(LocMesh))

# Using the estimated parameters from the model (see above)
# we can calculate the imposed Matern correlation values.
d.vec <- seq(0, max(D), length = 100)      
Cor.M <- (Kappa * d.vec) * besselK(Kappa * d.vec, 1) 
Cor.M[1] <- 1

# Which we plot here:
par(mfrow=c(1,1))
plot(x = d.vec, y = Cor.M, 
     type = "l", 
     xlab = "Distance", 
     ylab = "Correlation")
abline(h = 0.1, lty = 2)
```


```{r, eval=T, echo=F, fig.width=8, fig.height=4, dpi=600}
wproj <- inla.mesh.projector(mesh1a, xlim = range(Loc[,1]), ylim = range(Loc[,2])) 

wm.pm100100  <- inla.mesh.project(wproj, I1nb$summary.random$w$mean)
wsd.pm100100 <- inla.mesh.project(wproj, I1nb$summary.random$w$sd)
    
grid     <- expand.grid(x = wproj$x, y = wproj$y)
grid$zm  <- as.vector(wm.pm100100)   
grid$zsd <- as.vector(wsd.pm100100)   

wld <- map('world', xlim=c(-5,15), ylim=c(47,62),plot=FALSE)
wld <- data.frame(lon=wld$x, lat=wld$y)

UTMmap <- project(cbind(wld$lon, wld$lat), "+proj=utm +zone=31U ellps=WGS84")
UTMmapFinal <- data.frame("xm"  =UTMmap[,1]/1e3, "ym" = UTMmap[,2]/1e3)

p1 <- levelplot(zm ~ x * y  ,
          data = grid, 
          scales = list(draw = TRUE),
          xlab = list("Easting", cex = 1),
          ylab = list("Northing", cex = 1),
          main = list("Posterior mean spatial random fields", cex = 1),
          col.regions=tim.colors(25, alpha = 1),
          panel=function(x, y, z, subscripts,...){
            panel.levelplot(x, y, z, subscripts,...)
            grid.points(x = cpue_subset$Xkm,
                        y = cpue_subset$Ykm, 
                        pch = 1,
                        size = unit(cpue_subset$NoPerHaul/15, "char"))  
          }) +  xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black')

p2 <- levelplot(zsd ~ x * y,
          data = grid, 
          scales = list(draw = TRUE),
          xlab = list("Easting", cex = 1),
          ylab = list("Northing", cex = 1),
          main = list("Posterior sd spatial random fields", cex = 1),
          col.regions=tim.colors(25, alpha = 1),
          panel=function(x, y, z, subscripts,...){
            panel.levelplot(x, y, z, subscripts,...)
            
          }) +  xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black')

grid.arrange(p1,p2, ncol=2)
  
```


## Using swept area estimates
Rather than using time as a measure of effort, we will use surface. That will allow us to make predictions on the number of individuals caught per unit area. Because of the log link, the surfaces are log transformed (and named "lsurface"). Remember that the units of surface were km2. 

### Making the stack
Note that in code below the names in the model matrix should not contain any special characters! This is why we renamed the "NS-IBTS" survey to "NSIBTS" earlier.
```{r, eval=T, echo=T}
# 5. Make a stack. In this process we tell INLA at which points on the mesh we sampled the response variable and the covariates. 
Xmatrix <- model.matrix(~ fYear + fSurvey +  lsurface,  data = cpue_subset)
```

This Xmatrix contains the model matrix with the fixed effects, including the intercept (The column for the intercept is named "(Intercept)", and it is 1 for all observations). However, in the next step the intercept is removed from the model matrix. The intercept is then included when making the stack, and named "Intercept" (without brackets).

```{r, eval=T, echo=T}
X <- as.data.frame(Xmatrix[,-1])
names(X) <- c(gsub("[:]",".",names(X)))
head(X)

N <- nrow(cpue_subset)
Stack1 <- inla.stack(
    tag  = "Fit",
    data = list(y = cpue_subset$NoPerHaul),    
    A    = list(1,1, A1),                      
    effects = list( 
       Intercept=rep(1,N),
       X=X, #Covariates
       w=w.st))                  #Spatial field
```

Note that N is the number of rows (`r N`) in the data set, and thus equal to the first dimension of A1. 

### Making the model formula and running the INLA model

The model formula used in the inla model is generated from the names of the model matrix, combined with the intercept term and the spatial correlation model ("f(w, model=spde)").

Subsequently, two inla models are run, one assuming that the data are Poisson distributed, and another model assuming that the data are negative binomial distributed. These models are named I2nb and I2p. 

```{r, eval=T, results='hide', echo=T, message=FALSE, warning=FALSE}
fsp <- parse(text=c("y ~ -1 + Intercept + ",paste(c(names(X)," f(w, model = spde)"),collapse =" + ")))

INLA:::inla.dynload.workaround() 
I2p <- inla(eval(fsp), family = "poisson", data=inla.stack.data(Stack1),
           control.compute = list(dic = TRUE, waic = TRUE),
           control.predictor = list(A = inla.stack.A(Stack1)))

I2nb <- inla(eval(fsp), family = "nbinomial", data=inla.stack.data(Stack1),
           control.compute = list(dic = TRUE, waic = TRUE, config=TRUE),
           control.predictor = list(A = inla.stack.A(Stack1)))
```
Once the INLA models are run, the information criteria and summaries can be printed for the models. This summary contains much of the relevant information for the models, including the WAIC.

```{r, eval=T, echo=T}
dic2  <- c(I2p$dic$dic, I2nb$dic$dic)
waic2 <- c(I2p$waic$waic, I2nb$waic$waic)
Z     <- cbind(dic2, waic2)
rownames(Z) <- c("Poisson model",  "Negative binomial model" )
Z
```

```{r, eval=T, echo=T}
summary(I2nb)
```

We'll plot the spatial mean and sd, but this time with a grid size of 1 by 1 km. Because the sampling locations are on a km grid, we can first find the ranges of the x and y coordinates of the grid, and then have grid cells of size 1.
```{r, eval=T, echo=T, fig.width=8, fig.height=4, dpi=600}
# we want to make cells of 1 by 1 km. Let's do this first 
xl <- round(range(Loc[,1]))
xres <- xl[2]-xl[1] + 1
yl <-  round(range(Loc[,2]))
yres <- yl[2]-yl[1] + 1

wproj <- inla.mesh.projector(mesh1a, dims=c(xres,yres), xlim = xl, ylim = yl) 

wm.pm1km2  <- inla.mesh.project(wproj, I2nb$summary.random$w$mean)
wsd.pm1km2 <- inla.mesh.project(wproj, I2nb$summary.random$w$sd)
    
grid     <- expand.grid(x = wproj$x, y = wproj$y)
grid$zm  <- as.vector(wm.pm1km2)   
grid$zsd <- as.vector(wsd.pm1km2)   

wld <- map('world', xlim=c(-5,15), ylim=c(47,62),plot=FALSE)
wld <- data.frame(lon=wld$x, lat=wld$y)

UTMmap <- project(cbind(wld$lon, wld$lat), "+proj=utm +zone=31U ellps=WGS84")
UTMmapFinal <- data.frame("xm"  =UTMmap[,1]/1e3, "ym" = UTMmap[,2]/1e3)

p1 <- levelplot(zm ~ x * y  ,
          data = grid, 
          scales = list(draw = TRUE),
          xlab = list("Easting", cex = 1),
          ylab = list("Northing", cex = 1),
          main = list("Posterior mean spatial random fields", cex = 1),
          col.regions=tim.colors(25, alpha = 1),
          panel=function(x, y, z, subscripts,...){
            panel.levelplot(x, y, z, subscripts,...)
            grid.points(x = cpue_subset$Xkm,
                        y = cpue_subset$Ykm, 
                        pch = 1,
                        size = unit(cpue_subset$NoPerHaul/15, "char"))  
          }) +  xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black')

p2 <- levelplot(zsd ~ x * y,
          data = grid, 
          scales = list(draw = TRUE),
          xlab = list("Easting", cex = 1),
          ylab = list("Northing", cex = 1),
          main = list("Posterior sd spatial random fields", cex = 1),
          col.regions=tim.colors(25, alpha = 1),
          panel=function(x, y, z, subscripts,...){
            panel.levelplot(x, y, z, subscripts,...)
           
          }) +  xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black')

grid.arrange(p1,p2, ncol=2)
  
```


## Including depth in the swept area estimation.
Next we will test the effect of including the effect of depth on the counts as a random walk. Given the spatial patterns that were observed in the previous models, depth would be an ideal candidate to explain part of the spatial variation.

First, we need to add depth to the stack. 
```{r, eval=T, echo=T}
N <- nrow(cpue_subset)
Stack1 <- inla.stack(
    tag  = "Fit",
    data = list(y = cpue_subset$NoPerHaul),    
    A    = list(1,1,1, A1),                      
    effects = list( 
       Intercept=rep(1,N),
       X=X, #Covariates
       Depth = cpue_subset$Depth,
       w = w.st))                  #Spatial field
```

Next, we extend the model formula. Apart from the elements that we had earlier (the fixed effects and the spatial correlation), we now include the term for the random walk of depth. 
```{r, eval=T, echo=T}
#6.	Specify the model formula
fsp <- parse(text=c("y ~ -1 + Intercept + ",paste(c(names(X)," f(w, model = spde)", ' f(Depth,model="rw2")'),collapse =" + ")))
```

The new inla model is named I3nb (we only test a negative binomial error model).
```{r, eval=T, echo=T}
INLA:::inla.dynload.workaround() 
I3nb <- inla(eval(fsp), family = "nbinomial", data=inla.stack.data(Stack1),
           control.compute = list(dic = TRUE, waic = TRUE, config=TRUE, openmp.strategy="large"),
           control.predictor = list(A = inla.stack.A(Stack1)))

summary(I3nb)

```

The random walk of depth can be plotted using the code below:
```{r, eval=T, echo=T}
#plot depth smoother
Depthm <- I3nb$summary.random$Depth

par(mfrow = c(1,1))
plot(Depthm[,1:2], type='l',
     xlab = 'Depth', 
     ylab = 'Smoother',
     ylim = c(-3, 2) )
abline(h=0, lty=3)
lines(Depthm[, c(1, 4)], lty=2)
lines(Depthm[, c(1, 6)], lty=2)
rug(cpue_subset$Depth)
```

Did the spatial correlation change now that we included depth?
```{r, eval=T, echo=T, fig.width=8, fig.height=4, dpi=600}
# we want to make cells of 1 by 1 km. Let's do this first 
xl <- round(range(Loc[,1]))
xres <- xl[2]-xl[1] + 1
yl <-  round(range(Loc[,2]))
yres <- yl[2]-yl[1] + 1

wproj <- inla.mesh.projector(mesh1a, dims=c(xres,yres), xlim = xl, ylim = yl) 

wm.pm100100  <- inla.mesh.project(wproj, I3nb$summary.random$w$mean)
wsd.pm100100 <- inla.mesh.project(wproj, I3nb$summary.random$w$sd)
    
grid     <- expand.grid(x = wproj$x, y = wproj$y)
grid$zm  <- as.vector(wm.pm100100)   
grid$zsd <- as.vector(wsd.pm100100)   

wld <- map('world', xlim=c(-5,15), ylim=c(47,62),plot=FALSE)
wld <- data.frame(lon=wld$x, lat=wld$y)

UTMmap <- project(cbind(wld$lon, wld$lat), "+proj=utm +zone=31U ellps=WGS84")
UTMmapFinal <- data.frame("xm"  =UTMmap[,1]/1e3, "ym" = UTMmap[,2]/1e3)

p1 <- levelplot(zm ~ x * y  ,
          data = grid, 
          scales = list(draw = TRUE),
          xlab = list("Easting", cex = 1),
          ylab = list("Northing", cex = 1),
          main = list("Posterior mean spatial random fields", cex = 1),
          col.regions=tim.colors(25, alpha = 1),
          panel=function(x, y, z, subscripts,...){
            panel.levelplot(x, y, z, subscripts,...)
            grid.points(x = cpue_subset$Xkm,
                        y = cpue_subset$Ykm, 
                        pch = 1,
                        size = unit(cpue_subset$NoPerHaul/15, "char"))  
          }) +  xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black')

p2 <- levelplot(zsd ~ x * y,
          data = grid, 
          scales = list(draw = TRUE),
          xlab = list("Easting", cex = 1),
          ylab = list("Northing", cex = 1),
          main = list("Posterior sd spatial random fields", cex = 1),
          col.regions=tim.colors(25, alpha = 1),
          panel=function(x, y, z, subscripts,...){
            panel.levelplot(x, y, z, subscripts,...)
          }) +  xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black')

grid.arrange(p1,p2, ncol=2)
  
```

## Including spatio temporal-correlation in the swept area estimation with depth 

We need a new A1 object that contains not only the spatial correlation, but also the temporal correlation. This is done by creating a mesh for the time, where there is an observation for each year.
```{r, eval=TRUE, echo=TRUE}
# 2. Define the weighting factors a_ik (also called the projector matrix).
endyr <- max(cpue_subset$Year)
t.mesh <- inla.mesh.1d(loc = startyr:endyr)

A1     <- inla.spde.make.A(mesh = mesh1a,
                          loc   = Loc,
                          group = cpue_subset$Year,
                     group.mesh = t.mesh)

dim(A1)
```

The first dimension of the projector matrix has the size of the number of observations (here `r dim(A1)[1]`), and the second dimension of the projector matrix is the number of nodes in the mesh (here `r dim(A1)[2]`).

```{r, eval=TRUE, echo=TRUE}
# 3. Define the spde
spde  <- inla.spde2.matern(mesh1a)

w.st <- inla.spde.make.index('w', 
                             n.spde = spde$n.spde,
                             n.group = length(startyr:endyr))
```


The stack needs to be updated to include the new A1 object that now includes spatio temporal correlation instead of only spatial correlation.
```{r, eval=T, echo=T}
N <- nrow(cpue_subset)
Stack1 <- inla.stack(
    tag  = "Fit",
    data = list(y = cpue_subset$NoPerHaul),    
    A    = list(1,1,1, A1),                      
    effects = list( 
       Intercept=rep(1,N),
       X=X, #Covariates
       Depth = cpue_subset$Depth,
       w = w.st))                  #Spatial field
```

The model formula also needs to be updated.
```{r, eval=T, echo=T}
#6.	Specify the model formula
fsp <- parse(text=c("y ~ -1 + Intercept + ",paste(c(names(X)," f(w, model = spde, group = w.group, control.group = list(model = 'ar1'))", ' f(Depth,model="rw2")'), collapse =" + ")))
```

The new model is called I4nb.
```{r, eval=T, echo=T}
INLA:::inla.dynload.workaround() 
I4nb <- inla(eval(fsp), family = "nbinomial", data=inla.stack.data(Stack1),
           control.compute = list(dic = TRUE, waic = TRUE, config=TRUE, openmp.strategy="large"),
           control.predictor = list(A = inla.stack.A(Stack1)))

summary(I4nb)

```

Depth smoother can be plotted again.
```{r, eval=T, echo=T}
#plot depth smoother
Depthm <- I4nb$summary.random$Depth

par(mfrow = c(1,1), mar = c(5,5,2,2), cex.lab = 1.5)
plot(Depthm[,1:2], type='l',
     xlab = 'Depth', 
     ylab = 'Smoother',
     ylim = c(-2, 2) )
abline(h=0, lty=3)
lines(Depthm[, c(1, 4)], lty=2)
lines(Depthm[, c(1, 6)], lty=2)
rug(cpue_subset$Depth)
```

Now we plot the spatial correlation, but we get one panel per year
```{r, eval=T, echo=T, fig.width=8, fig.height=4, dpi=600}
# we want to make cells of 1 by 1 km. Let's do this first 
xl <- round(range(Loc[,1]))
xres <- xl[2]-xl[1] + 1
yl <-  round(range(Loc[,2]))
yres <- yl[2]-yl[1] + 1

years <- startyr:endyr

w.pm <- I4nb$summary.random$w$mean
wproj <- inla.mesh.projector(mesh1a, dims=c(xres,yres), xlim = xl, ylim = yl) 

grid <- expand.grid(year=years, x = wproj$x, y = wproj$y,zm=NA)

for (i in unique(w.st$w.group)){
    w.pm1km2 <- inla.mesh.project(wproj,w.pm[w.st$w.group==i])
    grid[grid$year==years[i],]$zm <- as.vector(w.pm1km2)  
}

wld <- map('world', xlim=c(-5,15), ylim=c(47,62),plot=FALSE)
wld <- data.frame(lon=wld$x, lat=wld$y)

UTMmap <- project(cbind(wld$lon, wld$lat), "+proj=utm +zone=31U ellps=WGS84")
UTMmapFinal <- data.frame("xm"  =UTMmap[,1]/1e3, "ym" = UTMmap[,2]/1e3)

p1 <- levelplot(zm ~ x * y|year  ,
          data = grid, 
          scales = list(draw = TRUE),
          xlab = list("Easting", cex = 1),
          ylab = list("Northing", cex = 1),
          main = list("Posterior mean spatial random fields", cex = 1),
          col.regions=tim.colors(25, alpha = 1),
          panel=function(x, y, z, subscripts,...){
            panel.levelplot(x, y, z, subscripts,...)
            grid.points(x = cpue_subset[cpue_subset$Year == grid[subscripts[1],]$year,]$Xkm,
                        y = cpue_subset[cpue_subset$Year == grid[subscripts[1],]$year,]$Ykm, 
                        pch = 1,
                        size = unit(cpue_subset[cpue_subset$Year ==grid[subscripts[1],]$year,]$NoPerHaul/25, "char"))  
          }) +  xyplot(ym~ xm, UTMmapFinal, type='l', lty=1, lwd=0.5, col='black')

plot(p1)

```

Next we plot the posterior of the estimated temporal correlation. The red vertical line indicates the mean of the distribution.
```{r, eval=T, echo=T, fig.width=8, fig.height=4, dpi=600}
rho <- I4nb$summary.hy[4,"mean"]
plot(I4nb$marginals.hyper[[4]],xlab= expression(rho),ylab="Density", type="l")
abline(v=rho, col=2)

```

Plot the characteristics of the spatial correlation.
```{r, eval=T, echo=T, fig.width=8, fig.height=4, dpi=600}

SpatField.w <- inla.spde2.result(inla = I4nb, name = "w", spde = spde, do.transfer = TRUE)

Kappa <- inla.emarginal(function(x) x, 
                        SpatField.w$marginals.kappa[[1]] )

Sigma_u <- inla.emarginal(function(x) sqrt(x), 
                        SpatField.w$marginals.variance.nominal[[1]] )

range <- inla.emarginal(function(x) x, 
                        SpatField.w$marginals.range.nominal[[1]] )

Kappa; Sigma_u; range       #Distance at which the correlation diminishes

# Show correlation structure
LocMesh <- mesh1a$loc[,1:2]

# And then we calculate the distance between each vertex.
D <- as.matrix(dist(LocMesh))

# Using the estimated parameters from the model (see above)
# we can calculate the imposed Matern correlation values.
d.vec <- seq(0, max(D), length = 100)      
Cor.M <- (Kappa * d.vec) * besselK(Kappa * d.vec, 1) 
Cor.M[1] <- 1

# Which we plot here:
par(mfrow=c(1,1))
plot(x = d.vec, y = Cor.M, 
     type = "l", 
     xlab = "Distance", 
     ylab = "Correlation")
abline(h = 0.1, lty = 2)
```


## Simlulate data by sampling from posterior (from I4nb)

Next we want to simulate a number realizations and integrate over surface (per year) so that we get a population level estimate.  

```{r, eval=T, echo=T}
# Simulate regression parameters using inla.posterior.sample 
# to simulate from the model. The output is stored in the Sim object.

set.seed(1234)
NSim <- 50
Sim <- inla.posterior.sample(n = NSim, result = I4nb)
```

In this example, we thus sample `r NSim` times. The Sim object is a list of length NSim. Each element of  this list contains a single realization. Let's have a closer look.

```{r, eval=T, echo=T}
names(Sim[[1]])

#get out names of different types of rows in latent
rnames <- rownames(Sim[[1]]$latent)
rtypes <- unique(unlist(lapply(strsplit(rnames,":"),function(x){x[1]})))
rtypes
```
Now, there are a number of different types of rows in latent: APredictor, Predictor, W, Depth, and the fixed effects.

```{r, eval=T, echo=T}
#to get the w par realizations out: 
rtypes[3]
wrownum <- grep(paste0("^",rtypes[3]),rownames(Sim[[1]]$latent))
wmat <- sapply(Sim, function(x) {x$latent[wrownum]})
dim(wmat)

#to get the depth realizations out: 
rtypes[4]
drownum <- grep(paste0("^",rtypes[4]),rownames(Sim[[1]]$latent))
dmat <- sapply(Sim, function(x) {x$latent[drownum]})
dim(dmat)


#to get the fixed effect par realizations out (1:4 now because depth was added) 
fixed <- rtypes[-(1:4)]
lrownum <- unlist(lapply(fixed, function(x) {grep(x, rownames(Sim[[1]]$latent), fixed = TRUE)}    ))
linmat <- sapply(Sim, function(x) {x$latent[lrownum]})
dim(linmat)
dimnames(linmat)[[1]] <- fixed
```
The second dimensions of wmat, dmat and linmat are equal to the number of simulations, here `r NSim`. The first dimension of wmat is equal to ... . The first dimension of dmat is equal to the number of depths in the original dataset, see for instance length(unique(cpue_subset$Depth)). We do not have all depths in the depth range in our original dataset, so some interpolation is required.

We'll use the approx function to do a linear interpolation using the approx function. In that function we can specify the x and y values for whcih we have information, and estimate the y values at each depth in the grid using the xout argument.

First, we'll need the depth at each grid cell. The locations are in the wproj object as wproj$x and wproj$y. However, the x and y in the gam model that we use are called Xkm and Ykm. Thus, we need some translation first.

A quick plot of the depths (with predictions between 0 and 250 m reveals that the depths are OK. Check if this is still the case when plotting the depths object (that is a matrix with sizes equal to wproj)
```{r, eval=T, echo=T}

fullgrid <- expand.grid(Xkm=wproj$x, Ykm = wproj$y )

fullgrid$depth <- predict(depthmodel, newdata=fullgrid) 
 levelplot(depth ~ Xkm * Ykm,  data = fullgrid, at=seq(0,250,10))
 
depths <- matrix(fullgrid$depth, nrow=length(wproj$x),ncol=length(wproj$y), dimnames = list("x" = wproj$x, "y" = wproj$y))

#constrain depths to be within min and max observed depths (otherwise interpolation will fail later)
depths <- apply(depths, c(1, 2), function(x) min(max(x,min(cpue_subset$Depth)),max(cpue_subset$Depth)))

#take only depths where our grid is (by using info from earlier plot)
depths[is.na(w.pm1km2)] <- NA 
image(depths)
```




length(wm.pm1km2[!is.na(wm.pm1km2)]) contains surface of grid in km2, because it was generated as a 1 by 1 km grid. 

```{r, eval=T, echo=T}

numbers <- matrix(NA, ncol=length( startyr:2017), nrow=NSim, dimnames=list("sim"=1:NSim,"year"=startyr:2017))

#we will need to extract correct group from spatiotemporal w. Those are numbered 1:xxx.
# we make a separate counter names gg starting at 1
gg <- 1

for (ii in startyr:2017){
  for (jj in 1:NSim){
               #intercept  +           year        +  ( 0 times beta for lsurface (so we get 1 km))
    if (ii==startyr){
      lin <- linmat["Intercept",jj] +                                 0 * linmat["lsurface",jj]
    }else{
      lin <- linmat["Intercept",jj] + linmat[paste0("fYear",ii),jj] + 0 * linmat["lsurface",jj]
    }
    
    # section below is for including spatial corr. Now we have to select the group. 
    wm.pm1km2  <- inla.mesh.project(wproj, wmat[w.st$w.group==gg,jj])

    # merging depth in?
    dd <- approx(x=unique(cpue_subset$Depth),y=dmat[,jj], xout=c(depths))$y
    dm.pm1km2 <-  matrix(dd, nrow=length(wproj$x),ncol=length(wproj$y), dimnames=list("x"=wproj$x,
                                                                                      "y"=wproj$y))

    #only take dm and wm values only where wm.pm1km2 non-NAs
    dm.pm1km2 <- dm.pm1km2[!is.na(wm.pm1km2)]
    wm.pm1km2 <- wm.pm1km2[!is.na(wm.pm1km2)]

    res <- exp(lin + wm.pm1km2 + dm.pm1km2)
    numbers[jj,as.character(ii)] <- sum(rnbinom(n = length(res), mu = res, size = I4nb$summary.hyperpar$mean[1]))
  }
  gg <- gg+1
}
```

Number is the estimated number for the entire surface of the grid
what are the conf bounds for numbers per year?

```{r, eval=T, echo=T}
qnumbers <- apply(numbers,2, quantile,c(0.025,0.5,0.975))
matplot(t(qnumbers), xlab="Years", ylab="Population numbers")
```

what are the average weights per year?
To go from number to weight we need alpha and beta from growth curve (for length in cm and resulting W is in g )
alpha=0.156650
beta=2.190 ref is Bedford et al. 1986

```{r, eval=T, echo=T}
alpha <- 0.156650
beta <- 2.190
full_cpue$wt <-  (alpha * (full_cpue$LngtClass/10)^beta * full_cpue$NoPerHaul) /1000
annualmnwt <- aggregate(cbind(wt,NoPerHaul) ~ Year, data= full_cpue,  FUN= "sum") 
annualmnwt <- within(annualmnwt, mnwt <- wt/NoPerHaul)

plot(mnwt~Year, data=annualmnwt, type="b")
```

Multiply weights by population numbers to get estimated population biomass. Divide by 1e6 to go from kg to 1000 tonnes. 

```{r, eval=T, echo=T}
qwts <- annualmnwt[annualmnwt$Year >= startyr,]$mnwt * qnumbers / 1e6
```


```{r, eval=T, echo=T}
plot(qwts[2,], xlab="Years", ylab="Estimated Total Stock Weight (1000 tonnes)", las=1, type="l", ylim=c(0,max(qwts)))
lines(qwts[1,], lty=2)
lines(qwts[3,], lty=2)
```


## Simlulate data by sampling from posterior, but now from simpler model (I2nb; without depth and only spatial corr)


```{r, eval=FALSE, echo=T}

# Simulate from model I1nb
set.seed(1234)
NSim <- 50
Sim <- inla.posterior.sample(n = NSim, result = I2nb)
```

In this example, we thus sample `r NSim` times. The Sim object is a list of length NSim. Each element of  this list contains a single realization. Let's have a closer look.

```{r, eval=T, echo=T}
names(Sim[[1]])

#get out names of different types of rows in latent
rnames <- rownames(Sim[[1]]$latent)
rtypes <- unique(unlist(lapply(strsplit(rnames,":"),function(x){x[1]})))
rtypes

```

```{r, eval=T, echo=T}
#to get the w par realizations out: 
rtypes[3]
wrownum <- grep(paste0("^",rtypes[3]),rownames(Sim[[1]]$latent))
wmat <- sapply(Sim, function(x) {x$latent[wrownum]})
dim(wmat)

#to get the fixed effect par realizations out 
fixed <- rtypes[-(1:3)]
lrownum <- unlist(lapply(fixed, function(x) {grep(x, rownames(Sim[[1]]$latent), fixed = TRUE)}    ))
linmat <- sapply(Sim, function(x) {x$latent[lrownum]})
dim(linmat)
dimnames(linmat)[[1]] <- fixed
```


```{r, eval=T, echo=T}

numbers <- matrix(NA, ncol=length( startyr:2017), nrow=NSim, dimnames=list("sim"=1:NSim,"year"=startyr:2017))

#we will need to extract correct group from spatiotemporal w. Those are numbered 1:xxx.
# we make a separate counter names gg starting at 1

for (ii in startyr:2017){
  for (jj in 1:NSim){
               #intercept  +           year        +  ( 0 times beta for lsurface (so we get 1 km))
    if (ii==startyr){
      lin <- linmat["Intercept",jj] +                                 0 * linmat["lsurface",jj]
    }else{
      lin <- linmat["Intercept",jj] + linmat[paste0("fYear",ii),jj] + 0 * linmat["lsurface",jj]
    }
    
    # section below is for including spatial corr. Now we have to select the group. 
    wm.pm1km2  <- inla.mesh.project(wproj, wmat[,jj])

    #only take wm values only where wm.pm1km2 non-NAs
    wm.pm1km2 <- wm.pm1km2[!is.na(wm.pm1km2)]

    res <- exp(lin + wm.pm1km2 )
    numbers[jj,as.character(ii)] <- sum(rnbinom(n = length(res), mu = res, size = I2nb$summary.hyperpar$mean[1]))
  }
}
```

Number is the estimated number for the entire surface of the grid
what are the conf bounds for numbers per year?

```{r, eval=T, echo=T}
qnumbers <- apply(numbers,2, quantile,c(0.025,0.5,0.975))
matplot(t(qnumbers), xlab="Years", ylab="Population numbers")
```


```{r, eval=T, echo=T}
qwts <- annualmnwt[annualmnwt$Year >= startyr,]$mnwt * qnumbers / 1e6
```


```{r, eval=T, echo=T}
plot(qwts[2,], xlab="Years", ylab="Estimated Total Stock Weight (1000 tonnes)", las=1, type="l", ylim=c(0,max(qwts)))
lines(qwts[1,], lty=2)
lines(qwts[3,], lty=2)
```

